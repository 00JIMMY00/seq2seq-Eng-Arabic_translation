{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing library\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils import *\n",
    "from keras.initializers import *\n",
    "import tensorflow as tf\n",
    "# import time, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize the data.\n",
    "num_samples = 10000\n",
    "\n",
    "input_texts = []\n",
    "validation_inputs = []\n",
    "target_texts = []\n",
    "validation_target = []\n",
    "input_chars = set()\n",
    "target_chars = set()\n",
    "\n",
    "with open(r'C:\\Users\\ahmed\\Downloads\\seq2seqTranslation_arabic\\ara.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    input_texts.append(input_text)\n",
    "    target_text = target_text.lower()\n",
    "    target_texts.append(target_text)\n",
    "\n",
    "for line in lines[  min(num_samples, len(lines) - 1):]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    validation_inputs.append(input_text)\n",
    "    target_text = target_text.lower()\n",
    "    validation_target.append(target_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unicode normalization\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def normalize_unicode(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence_eng(s):\n",
    "    s = normalize_unicode(s)\n",
    "    s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
    "    s = re.sub(r'[\" \"]+', \" \", s)\n",
    "    s = re.sub(r'[^a-z A-Z 0-9\\s]+', \"\", s)\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "def preprocess_sentence_arabic(s):\n",
    "    s = normalize_unicode(s)\n",
    "    \n",
    "    # Arabic-specific punctuation\n",
    "    arabic_punctuation = r'[،؟؛]'  # Original Arabic punctuation marks\n",
    "    \n",
    "    # Common punctuation (including the dot, comma, exclamation, etc.)\n",
    "    common_punctuation = r'[.،؟؛!,]'  # Add the dot '.' here and other common punctuation marks\n",
    "    \n",
    "    # Diacritics (Tashkeel)\n",
    "    tashkeel = r'[\\u064B-\\u0652]'  # Arabic diacritical marks\n",
    "    \n",
    "    # Combine patterns to remove tashkeel and punctuation\n",
    "    combined_pattern = tashkeel + '|' + common_punctuation\n",
    "    \n",
    "    # Remove tashkeel and punctuation\n",
    "    s = re.sub(combined_pattern, '', s)\n",
    "    \n",
    "    # Strip extra spaces\n",
    "    s = s.strip()\n",
    "    \n",
    "    return s\n",
    "\n",
    "\n",
    "def tag_target_sentences(sentences):\n",
    "    tagged_sentences = map(lambda s: (' ').join(['<sos>', s, '<eos>']), sentences)\n",
    "    return list(tagged_sentences)\n",
    "\n",
    "def generate_decoder_inputs_targets(sentences, tokenizer):\n",
    "    seqs = tokenizer.texts_to_sequences(sentences)\n",
    "    decoder_inputs = [s[:-1] for s in seqs] # Drop the last token in the sentence.\n",
    "    decoder_targets = [s[1:] for s in seqs] # Drop the first token in the sentence.\n",
    "    return decoder_inputs, decoder_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the data\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "English_Tokenizer = Tokenizer(oov_token='<unk>', filters='\"#$%&()*+-/:;=@[\\\\]^_`{|}~\\t\\n')\n",
    "\n",
    "\n",
    "def preprocess_encoder_inputs(input_texts, tokenizer):\n",
    "    train_preprocessed_input = [preprocess_sentence_eng(s) for s in input_texts]\n",
    "    train_tagged_preprocessed_input = tag_target_sentences(train_preprocessed_input)\n",
    "\n",
    "    tokenizer.fit_on_texts(train_tagged_preprocessed_input)\n",
    "\n",
    "    input_vocab_size = len(English_Tokenizer.index_word)+1\n",
    "    train_encoder_inputs = English_Tokenizer.texts_to_sequences(train_tagged_preprocessed_input)\n",
    "    max_encoding_len = len(max(train_encoder_inputs, key=len))\n",
    "\n",
    "    padded_train_encoder_inputs = pad_sequences(train_encoder_inputs, max_encoding_len, padding='post', truncating='post')\n",
    "    return padded_train_encoder_inputs,tokenizer, input_vocab_size, max_encoding_len\n",
    "\n",
    "def preprocess_decoder(target_text, tokenizer):\n",
    "    train_preprocessed_target = [preprocess_sentence_arabic(s) for s in target_texts]\n",
    "    train_tagged_preprocessed_target = tag_target_sentences(train_preprocessed_target)\n",
    "\n",
    "    tokenizer = Tokenizer(oov_token='<unk>', filters='\"#$%&()*+-/:;=@[\\\\]^_`{|}~\\t\\n')\n",
    "    tokenizer.fit_on_texts(train_tagged_preprocessed_target)\n",
    "\n",
    "    train_decoder_inputs, train_decoder_targets = generate_decoder_inputs_targets(train_tagged_preprocessed_target,\n",
    "tokenizer)\n",
    "    \n",
    "    max_decoding_len = len(max(train_decoder_inputs, key=len))\n",
    "\n",
    "    padded_train_decoder_inputs = pad_sequences(train_decoder_inputs, max_decoding_len, padding='post', truncating='post')\n",
    "    padded_train_decoder_targets = pad_sequences(train_decoder_targets, max_decoding_len, padding='post', truncating='post')\n",
    "\n",
    "    target_vocab_size = len(tokenizer.word_index) + 1\n",
    "    return padded_train_decoder_inputs,tokenizer, padded_train_decoder_targets , target_vocab_size, max_decoding_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the training data\n",
    "English_Tokenizer = Tokenizer(oov_token='<unk>', filters='\"#$%&()*+-/:;=@[\\\\]^_`{|}~\\t\\n')\n",
    "arabic_tokenizer = Tokenizer(oov_token='<unk>', filters='\"#$%&()*+-/:;=@[\\\\]^_`{|}~\\t\\n')\n",
    "\n",
    "padded_train_encoder_inputs,English_Tokenizer, input_vocab_size, max_encoding_len = preprocess_encoder_inputs(input_texts, English_Tokenizer)\n",
    "padded_train_decoder_inputs,arabic_tokenizer, padded_train_decoder_targets , target_vocab_size, max_decoding_len = preprocess_decoder(target_text, arabic_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8637"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(arabic_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving tokenizer as pickle \n",
    "import pickle\n",
    "\n",
    "with open('English_Tokenizer.pickle','wb') as handle:\n",
    "    pickle.dump(English_Tokenizer,handle,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('arabic_tokenizer.pickle','wb') as handle:\n",
    "    pickle.dump(arabic_tokenizer,handle,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the val data\n",
    "val_padded_encoder_inputs,_, val_input_vocab_size, val_max_encoding_len = preprocess_encoder_inputs(input_texts, English_Tokenizer)\n",
    "val_padded_decoder_inputs,_ , val_padded_train_decoder_targets , val_target_vocab_size, val_max_decoding_len = preprocess_decoder(target_text, arabic_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "default_dropout=0.2\n",
    "batch_size = 32\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_22 (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_23 (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " masking_17 (Masking)           (None, None)         0           ['input_22[0][0]']               \n",
      "                                                                                                  \n",
      " masking_18 (Masking)           (None, None)         0           ['input_23[0][0]']               \n",
      "                                                                                                  \n",
      " embedding_7 (Embedding)        (None, None, 128)    465920      ['masking_17[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_8 (Embedding)        (None, None, 128)    1105664     ['masking_18[0][0]']             \n",
      "                                                                                                  \n",
      " lstm_15 (LSTM)                 [(None, 256),        394240      ['embedding_7[0][0]']            \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " lstm_16 (LSTM)                 [(None, None, 256),  394240      ['embedding_8[0][0]',            \n",
      "                                 (None, 256),                     'lstm_15[0][1]',                \n",
      "                                 (None, 256)]                     'lstm_15[0][2]']                \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, None, 8638)   2219966     ['lstm_16[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,580,030\n",
      "Trainable params: 4,580,030\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Encoder model with Masking\n",
    "\n",
    "encoder_inputs = tf.keras.Input(shape=(None,))\n",
    "encoder_masking = layers.Masking(mask_value=0.0)(encoder_inputs)\n",
    "encoder_embedding = layers.Embedding(input_dim=input_vocab_size, output_dim=embedding_dim)(encoder_masking)\n",
    "encoder_lstm = layers.LSTM(hidden_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder model with Masking\n",
    "decoder_inputs = tf.keras.Input(shape=(None,))\n",
    "decoder_masking = layers.Masking(mask_value=0.0)(decoder_inputs)\n",
    "decoder_embedding = layers.Embedding(input_dim=target_vocab_size, output_dim=embedding_dim)(decoder_masking)\n",
    "decoder_lstm = layers.LSTM(hidden_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = layers.Dense(target_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Full model\n",
    "model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 34s 109ms/step - loss: 0.0440 - accuracy: 0.9856 - val_loss: 0.0327 - val_accuracy: 0.9896\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 34s 110ms/step - loss: 0.0424 - accuracy: 0.9858 - val_loss: 0.0333 - val_accuracy: 0.9898\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 34s 109ms/step - loss: 0.0420 - accuracy: 0.9860 - val_loss: 0.0324 - val_accuracy: 0.9896\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 35s 112ms/step - loss: 0.0411 - accuracy: 0.9857 - val_loss: 0.0307 - val_accuracy: 0.9899\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 33s 107ms/step - loss: 0.0400 - accuracy: 0.9859 - val_loss: 0.0308 - val_accuracy: 0.9897\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 36s 116ms/step - loss: 0.0387 - accuracy: 0.9860 - val_loss: 0.0284 - val_accuracy: 0.9900\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.0389 - accuracy: 0.9857 - val_loss: 0.0293 - val_accuracy: 0.9899\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.0381 - accuracy: 0.9861 - val_loss: 0.0300 - val_accuracy: 0.9897\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 36s 115ms/step - loss: 0.0382 - accuracy: 0.9862 - val_loss: 0.0272 - val_accuracy: 0.9903\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 37s 119ms/step - loss: 0.0355 - accuracy: 0.9866 - val_loss: 0.0268 - val_accuracy: 0.9902\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit([padded_train_encoder_inputs, padded_train_decoder_inputs],\n",
    "                    np.expand_dims(padded_train_decoder_targets, -1),\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=11,\n",
    "                    validation_data=([val_padded_encoder_inputs, val_padded_decoder_inputs],\n",
    "                                    np.expand_dims(val_padded_train_decoder_targets, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"eng_to_arabic_96_acc_97_val.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bulding the encoder and decoder \n",
    "encoder_model = tf.keras.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder inference model\n",
    "decoder_state_input_h = tf.keras.Input(shape=(hidden_dim,))\n",
    "decoder_state_input_c = tf.keras.Input(shape=(hidden_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_lstm_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_embedding, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_lstm_outputs)\n",
    "\n",
    "decoder_model = tf.keras.Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "encoder_model.save(\"eng_to_arabic_encoder_98_acc_99_val_v2.keras\")\n",
    "decoder_model.save(\"eng_to_arabic_decoder_98_acc_99_val_v2.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_decoding_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, english_tokenizer, arabic_tokenizer, encoder_model, decoder_model, max_encoding_len, max_decoding_len):\n",
    "    # Tokenize the input English sentence using english_tokenizer\n",
    "    input_seq = english_tokenizer.texts_to_sequences([sentence])\n",
    "    \n",
    "    # Pad the tokenized input to max_encoding_len\n",
    "    input_seq = pad_sequences(input_seq, maxlen=max_encoding_len, padding='post')\n",
    "\n",
    "    # Encode the input as state vectors using the encoder model\n",
    "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
    "\n",
    "    # Generate an empty target sequence of length 1\n",
    "    target_seq = np.zeros((1, 1))  # [[0]]\n",
    "\n",
    "    # Populate the first character of the target sequence with the start token ('<sos>')\n",
    "    target_seq[0, 0] = arabic_tokenizer.word_index['<sos>']\n",
    "\n",
    "    # Sampling loop to generate the French sentence\n",
    "    stop_condition = False\n",
    "    translated_sentence = ''\n",
    "    \n",
    "    while not stop_condition:\n",
    "        # Predict the next token and hidden states (h, c) from the decoder model\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=0)\n",
    "\n",
    "        # Sample the token with the highest probability\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = arabic_tokenizer.index_word.get(sampled_token_index, '<unk>')\n",
    "\n",
    "        # Append the sampled word to the translated sentence\n",
    "        translated_sentence += ' ' + sampled_word\n",
    "\n",
    "        # Stop if we encounter the end token ('<eos>') or exceed max_decoding_len\n",
    "        if sampled_word == '<eos>' or len(translated_sentence.split()) > max_decoding_len:\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            # Update the target sequence with the sampled token\n",
    "            target_seq = np.zeros((1, 1))\n",
    "            target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "            # Update the states (h, c) for the next prediction\n",
    "            states_value = [h, c]\n",
    "\n",
    "    # Return the translated sentence without '<eos>' token\n",
    "    translated_sentence = translated_sentence.replace('<eos>', '').strip()\n",
    "    return translated_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'اريد ان اعرف رايك'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_sentence(\"I want to know your opinion\",English_Tokenizer, arabic_tokenizer, encoder_model, decoder_model,max_encoding_len, max_encoding_len  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
